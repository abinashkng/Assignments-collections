{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyP9MEo97/iNoPuExOS8j6Cn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abinashkng/Assignments-collections/blob/main/Data_clip.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests beautifulsoup4 pandas"
      ],
      "metadata": {
        "id": "BBxPxGdwb6fM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xT7I-phXbv2X"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# Function to get the HTML content of the page\n",
        "def get_html(url):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        return response.text\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Function to parse the HTML and extract job details\n",
        "def parse_html(html):\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "    jobs = []\n",
        "    for job_card in soup.find_all('div', class_='jobTuple bgWhite br4 mb-8'):\n",
        "        title = job_card.find('a', class_='title fw500 ellipsis').text.strip()\n",
        "        company = job_card.find('a', class_='subTitle ellipsis fleft').text.strip()\n",
        "        location = job_card.find('li', class_='fleft grey-text br2 placeHolderLi location').text.strip()\n",
        "        jobs.append({'Title': title, 'Company': company, 'Location': location})\n",
        "    return jobs\n",
        "\n",
        "def scrape_naukri_jobs(search_query, location):\n",
        "    base_url = 'https://www.naukri.com/'\n",
        "    search_url = f'{base_url}{search_query}-jobs-in-{location}'\n",
        "    html = get_html(search_url)\n",
        "    if html:\n",
        "        jobs = parse_html(html)\n",
        "        return jobs\n",
        "    else:\n",
        "        return []\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    search_query = 'CA, CS'\n",
        "    location = 'Delhi NCR'\n",
        "    jobs = scrape_naukri_jobs(search_query, location)\n",
        "    df = pd.DataFrame(jobs)\n",
        "    print(df)\n",
        "    df.to_csv('naukri_jobs.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install selenium webdriver-manager"
      ],
      "metadata": {
        "id": "4SX6IxpIc8KJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.chrome.service import Service as ChromeService\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "\n",
        "# Set up Chrome options\n",
        "options = Options()\n",
        "\n",
        "options.binary_location = '/usr/bin/google-chrome'\n",
        "\n",
        "# Set up the driver\n",
        "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()), options=options)\n",
        "driver.get('https://www.naukri.com/company-secretary-cs-jobs?k=company%20secretary%2C%20cs%2C%20cs&nignbevent_src=jobsearchDeskGNB&cityTypeGid=9508&jobAge=7')\n",
        "\n",
        "# Giving time to load\n",
        "driver.implicitly_wait(10)\n",
        "\n",
        "# Extract job listings\n",
        "job_listings = driver.find_elements(By.CLASS_NAME, 'jobTuple')\n",
        "\n",
        "for job in job_listings:\n",
        "    title = job.find_element(By.CLASS_NAME, 'title').text.strip()\n",
        "    company = job.find_element(By.CLASS_NAME, 'subTitle').text.strip()\n",
        "    location = job.find_element(By.CLASS_NAME, 'location').text.strip()\n",
        "\n",
        "    print(f'Job Title: {title}')\n",
        "    print(f'Company: {company}')\n",
        "    print(f'Location: {location}')\n",
        "    print('-' * 20)\n",
        "\n",
        "driver.quit()"
      ],
      "metadata": {
        "id": "j3bNZ0FGdBeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "   import requests\n",
        "   from bs4 import BeautifulSoup\n",
        "\n",
        "   url = 'https://www.naukri.com/company-secretary-cs-jobs?k=company%20secretary%2C%20cs%2C%20cs&nignbevent_src=jobsearchDeskGNB&cityTypeGid=9508&jobAge=7'\n",
        "   response = requests.get(url)\n",
        "   soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "   job_listings = soup.find_all('div', class_='jobTuple bgWhite br4 mb-8')\n",
        "\n",
        "   for job in job_listings:\n",
        "       title = job.find('a', class_='title fw500 ellipsis').text.strip()\n",
        "       company = job.find('a', class_='subTitle ellipsis fleft').text.strip()\n",
        "       location = job.find('li', class_='fleft grey-text br2 placeHolderLi location').text.strip()\n",
        "       print(f'Job Title: {title}')\n",
        "       print(f'Company: {company}')\n",
        "       print(f'Location: {location}')\n",
        "       print('-' * 20)"
      ],
      "metadata": {
        "id": "SEJz_tqld9xV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "   !pip install playwright\n",
        "   from playwright.async_api import async_playwright\n",
        "\n",
        "   async def main():\n",
        "       async with async_playwright() as p:\n",
        "           browser = await p.chromium.launch()\n",
        "           page = await browser.new_page()\n",
        "           await page.goto('https://www.naukri.com/company-secretary-cs-jobs?k=company%20secretary%2C%20cs%2C%20cs&nignbevent_src=jobsearchDeskGNB&cityTypeGid=9508&jobAge=7')\n",
        "\n",
        "           await browser.close()\n",
        "\n",
        "   import asyncio\n",
        "   asyncio.run(main())"
      ],
      "metadata": {
        "id": "72KYyIgJeZVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ahnaBuosJCha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "url = 'https://www.naukri.com/company-secretary-cs-jobs?k=company%20secretary%2C%20cs%2C%20cs&nignbevent_src=jobsearchDeskGNB&cityTy'\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "# Check if the element exists before accessing its attributes\n",
        "element_by_id = soup.find(id='my_element_id')\n",
        "if element_by_id:  # If element_by_id is not None\n",
        "    text = element_by_id.text\n",
        "else:\n",
        "    text = \"Element not found\"  # Or handle it differently\n",
        "\n",
        "# Check if the elements_by_class list is not empty before accessing its elements\n",
        "elements_by_class = soup.find_all(class_='my_element_class')\n",
        "if elements_by_class:\n",
        "    attribute_value = elements_by_class[0]['href']\n",
        "else:\n",
        "    attribute_value = \"No elements found with this class\"  # Or handle it differently\n",
        "\n",
        "elements_by_tag = soup.find_all('p')\n",
        "\n",
        "# Print or use the extracted data\n",
        "print(text)\n",
        "print(attribute_value)"
      ],
      "metadata": {
        "id": "xG83Z250JHox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L3elHOepJeng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def scrape_with_beautifulsoup(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Find the specific <a> tag using its attributes\n",
        "    a_tag = soup.find('a', href=\"https://www.google.co.in/setprefs?sig=0_eQAxFIVLXPRAgNXkv7-LUK5FP8w%3D&hl=bn&source=homepage&sa=X&ved=0ahUKEwjl9Max-6SJAxW71zgGHSUcAIYQ2ZgBCCU\")\n",
        "\n",
        "    if a_tag:\n",
        "        text_inside_a_tag = a_tag.text.strip()\n",
        "        print(f\"Text inside <a> tag (BeautifulSoup): {text_inside_a_tag}\")\n",
        "    else:\n",
        "        print(\"The <a> tag was not found.\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    url = 'https://www.google.com/'\n",
        "    scrape_with_beautifulsoup(url)"
      ],
      "metadata": {
        "id": "_SbISpRoMc46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install playwright"
      ],
      "metadata": {
        "id": "IONeX4J1RlJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!playwright install"
      ],
      "metadata": {
        "id": "rJXWBEubSVEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install playwright\n",
        "!playwright install\n",
        "\n",
        "from playwright.async_api import async_playwright\n",
        "\n",
        "async def scrape_with_playwright(url):\n",
        "    async with async_playwright() as p:\n",
        "        browser = await p.chromium.launch()\n",
        "        page = await browser.new_page()\n",
        "        await page.goto(url)\n",
        "\n",
        "        # Use a CSS selector to target the <a> tag\n",
        "        a_tag_text = await page.locator('a[href=\"https://www.google.co.in/setprefs?sig=0_eQAxFIVLXPRAgNXkv7-LUK5FP8w%3D&hl=bn&source=homepage&sa=X&ved=0ahUKEwjl9Max-6SJAxW71zgGHSUcAIYQ2ZgBCCU\"]').text_content()\n",
        "\n",
        "        print(f\"Text inside <a> tag (Playwright): {a_tag_text}\")\n",
        "        await browser.close()\n",
        "\n",
        "\n",
        "await scrape_with_playwright('https://www.google.com/')"
      ],
      "metadata": {
        "id": "UuyvnPewRikV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"PySparkExample\").getOrCreate()\n",
        "\n",
        "# Create a DataFrame\n",
        "data = [(1, \"Alice\"), (2, \"Bob\"), (3, \"Charlie\")]\n",
        "columns = [\"id\", \"name\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Show DataFrame\n",
        "df.show()\n"
      ],
      "metadata": {
        "id": "EF4NIWLZw5Yf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import dask.dataframe as dd\n",
        "import pandas as pd\n",
        "\n",
        "# Create a Pandas DataFrame\n",
        "pdf = pd.DataFrame({\"A\": range(100000000), \"B\": range( 100000000)})\n",
        "\n",
        "# Convert to a Dask DataFrame with 2 partitions\n",
        "ddf = dd.from_pandas(pdf, npartitions=2)\n",
        "\n",
        "# Compute and display the DataFrame\n",
        "print(ddf.compute())\n"
      ],
      "metadata": {
        "id": "yhBvsfvmynf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import dask.dataframe as dd\n",
        "import pandas as pd\n",
        "\n",
        "# Create a Pandas DataFrame\n",
        "pdf = pd.DataFrame({\"A\": range(100000000), \"B\": range(100000000)})\n",
        "\n",
        "# Convert to a Dask DataFrame with 2 partitions\n",
        "\n",
        "# Compute and display the DataFrame\n",
        "print(pdf)\n"
      ],
      "metadata": {
        "id": "gwtae3vkyzst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vaex"
      ],
      "metadata": {
        "id": "dhf_Lo3B0MBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import vaex\n",
        "\n",
        "# Create a Vaex DataFrame from arrays\n",
        "df = vaex.from_arrays(a=range(10), b=range(10, 20))\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "print(df.head(5))\n"
      ],
      "metadata": {
        "id": "a3dvQ4uP0Jzl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}